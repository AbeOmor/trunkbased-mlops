name: Comparing models
description: 'Compares two model versions, a champion and a challenger, based on a given metric and indicates if the challenger model, acting a the new model, is better than the current champion, acting as the current deployed model. This method is usually called champion/challenger comparison. Metrics of the models should be logged in the run that generated them. This implies that model registration should always indicate the Run ID that generated the model.'

inputs:
  modelName:
    description: 'Name of the model to compare'
    required: true
  champion:
    description: 'Version of the model that is acting as the champion. It can be indicated using a given model version, like 22, or using tokens like `current` indicating the current deployed version in a given endpoint/deployment. If indicated with `current`, then `endpoint` parameter should be specified. If `current` is indicated and there is no model deployed in `endpoint` then the comparison will always vote for challenger.'
    required: true
  challenger:
    description: 'Version of the model that is acting as the challenger. It can be indicated using a given model version, like 22, or using tokens like `latest` indicating the latest version of the model in the registry.'
    required: true
    default: 'latest'
  deployment:
    description: 'The name of the deployment inside the given endpoint to get the model from. If this parameter is not indicated but an endpoint has been specified, then the first available endpoint is used inside the endpoint.'
    required: true
  compareBy:
    description: 'The name of the metric used to compare the models. This metric should be logged in the runs that generated the given models. Defaults to `accuracy`.'
    required: true
    default: 'accuracy'
  greaterIsBetter:
    description: 'Indicates if greater values of the metric `compareBy` are better.'
    required: true
    default: 'true'
  workspaceName:
    description: 'Name of the workspace to work against.'
    required: true
  resourceGroup:
    description: 'Name of the resource group where the workspace is placed.'
    required: true

outputs:
  result:
    description: ' Either `true` or `false` depending if the comparison favored `challenger` over `champion`.'
    value: ${{ steps.compare.outputs.result }}


runs:
  using: "composite"
  steps:
    - name: Comparing models
      id: compare
      shell: bash
      run: |        
        echo "::debug::Comparing models by ${{ inputs.compareBy }}"

        SUBSCRIPTION_ID=$(az account show | jq -r ".id")

        COMPARE=$(python ${{ github.action_path }}/compare.py \
          --subscription-id $SUBSCRIPTION_ID \
          --workspace-name ${{ inputs.workspaceName }} \
          --resource-group ${{ inputs.resourceGroup }} \
          --model-name ${{ inputs.modelName }} \
          --champion $CHAMPION \
          --challenger $CHALLENGER \
          --compare-by ${{ inputs.compareBy }} \
          --greater-is-better ${{ inputs.greaterIsBetter }})

        # compare.py will return True or False, but bash expects lowercase.
        if ${COMPARE,,} ; then
          echo "::debug::Challenger model won the compare and will be deployed."
          echo "::set-output name=result::true"
        else
          echo "::warning::Challenger model is not better than the current champion."
          echo "::set-output name=result::false"
        fi